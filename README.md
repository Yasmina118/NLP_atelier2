# NLP_atelier2
## Part 1:Rule Based NLP and Regex:
In the first part of our lab, we explored the application of regular expressions (regex) in Python for text processing tasks. This exercise provided us with a practical understanding of how regex can be utilized to extract and manipulate numerical data embedded within text. We developed a set of functions designed to handle specific data extraction scenarios like
Extracting Numbers,Word to Number Conversion and Filtering Specific Words Following Number ...
The integration of these functions into a user-interactive script demonstrated the practical application of regex in processing and interpreting user input to extract a bill from text entered by the user. This part of the lab effectively combined all the functions to calculate total prices from extracted quantities and unit prices, presenting the results in a tabular format.
## Part 2: word Embedding :
In this lab, we discovered the world of word embeddings, a fundamental concept in natural language processing that allows for numerical representation of textual data. We began by applying simpler, traditional techniques such as one-hot encoding, Bag of Words (BoW), and TF-IDF. These methods provided a foundational understanding of how text can be vectorized, highlighting the importance of word frequency and document relevance.

Following these initial methods, we explored more advanced embedding techniques that consider the context of words within the corpus. We applied Word2Vec approaches, including both Skip-Gram and CBOW models, which construct word associations based on their surrounding context. We then extended our exploration to GloVe, which integrates the benefits of global matrix factorization and local context window techniques, offering a robust framework for generating word embeddings from word co-occurrence statistics.

Additionally, we experimented with FastText, an extension of the Word2Vec model that incorporates subword information, thus enhancing the representation of rare words and morphologically rich languages.

To visually assess the quality and characteristics of the embeddings generated by these various methods, we employed the t-SNE algorithm for dimensionality reduction. This visualization technique allowed us to observe the clustering of similar words and the separation of dissimilar ones, providing intuitive insights into the semantic and syntactic capabilities of each embedding technique.

Through this comprehensive exploration, we discovered the diverse approaches to word embedding and their respective strengths and weaknesses. From the sparsity and simplicity of one-hot encoding to the nuanced and contextually rich embeddings produced by GloVe and FastText, this lab has underscored the evolution of text representation techniques and their pivotal role in advancing NLP applications.

